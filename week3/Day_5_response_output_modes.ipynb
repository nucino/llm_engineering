{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Response Output: Full Block vs. Low Level Streaming Mode vs. High Level Streaming Mode\n",
        "\n",
        "This notebook can run on a low-cost or free T4 runtime."
      ],
      "metadata": {
        "id": "aKs1PM-O-VQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate gradio"
      ],
      "metadata": {
        "id": "NthhKJRwX1iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9zvDGWD5pKp"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, TextIteratorStreamer\n",
        "import torch\n",
        "import gradio as gr\n",
        "import threading"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sign in to Hugging Face"
      ],
      "metadata": {
        "id": "xyKWKWSw7Iqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "xd7cEDUC6Lkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the instruct model names\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "UtN7OKILQato"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Different types of Response Output"
      ],
      "metadata": {
        "id": "bkK9dcIwt51v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define helper function to load model and tokenizer\n",
        "def load_model(model_name):\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "      bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quant_config)\n",
        "  return tokenizer, model"
      ],
      "metadata": {
        "id": "PF9B6frmubPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define different generating functions:\n",
        "#   1- full response\n",
        "#   2- low level streaming response\n",
        "#   3- low level streaming response\n",
        "\n",
        "def generate_full(tokenizer, model, user_input, max_tokens=2000):\n",
        "  global messages\n",
        "  # Append the user's new message to the conversation history\n",
        "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  outputs = model.generate(inputs, max_new_tokens=max_tokens)\n",
        "  response = tokenizer.decode(outputs[0])\n",
        "  print(response)\n",
        "\n",
        "def generate_stream_low_level(tokenizer, model, user_input, max_tokens=2000):\n",
        "    global messages\n",
        "    # Append the user's new message to the conversation history\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Prepare the initial input\n",
        "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "\n",
        "    # Generate up to 2000 tokens\n",
        "    for _ in range(max_tokens):\n",
        "        outputs = model(input_ids)  # Get the model's output (logits) for the given input IDs\n",
        "        # Select the token with the highest probability from the last position's logits\n",
        "        next_token_id = outputs.logits[:, -1].argmax(dim=-1).unsqueeze(-1)\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)  # Append new token\n",
        "        next_token = tokenizer.decode(next_token_id[0])  # Decode and print\n",
        "        # flush=True ensures the output is immediately written to the console.\n",
        "        # By default, print output is buffered, so it may not appear instantly.\n",
        "        # flush=True forces the buffer to flush, making real-time output possible.\n",
        "        print(next_token, end=\"\", flush=True)\n",
        "\n",
        "        if next_token_id.item() == tokenizer.eos_token_id:  # Stop if EOS token\n",
        "            break\n",
        "    print()\n",
        "\n",
        "def generate_stream_high_level(tokenizer, model, user_input, max_tokens=2000):\n",
        "  global messages\n",
        "  # Append the user's new message to the conversation history\n",
        "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  # we skip using TextStreamer() here cause it streams back results to stdout and thats not what we want in gradio app\n",
        "  # and we use TextIteratorStreamer() instead\n",
        "\n",
        "  # Initialize the TextIteratorStreamer for streaming output\n",
        "  streamer = TextIteratorStreamer(\n",
        "      tokenizer,\n",
        "      skip_prompt=True,\n",
        "      decode_kwargs={\"skip_special_tokens\": True}\n",
        "  )\n",
        "\n",
        "  # Run the generation process in a separate thread\n",
        "  thread = threading.Thread(\n",
        "      target=model.generate,\n",
        "      kwargs={\"inputs\": inputs, \"max_new_tokens\": max_tokens, \"streamer\": streamer}\n",
        "  )\n",
        "  thread.start()\n",
        "\n",
        "  # Stream and print the output progressively\n",
        "  for text_chunk in streamer:\n",
        "    filtered_chunk = text_chunk.replace(\"<|eot_id|>\", \"\")  # Remove special tokens if present\n",
        "    print(filtered_chunk, end=\"\")  # Print without adding new lines"
      ],
      "metadata": {
        "id": "gIfSfTXftPp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model and tokenizer in order to test generating"
      ],
      "metadata": {
        "id": "TuJ7stpBu6zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call the helper function and load the model and tokenizer\n",
        "tokenizer, model = load_model(LLAMA)"
      ],
      "metadata": {
        "id": "Xktx0cLAubLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start testing the three generating functions"
      ],
      "metadata": {
        "id": "SX3_25npwx5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the messages history, the max tokens for the model, and the user_input\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}\n",
        "]\n",
        "\n",
        "max_tokens = 2000\n",
        "\n",
        "user_input = \"What is the meaning of life? Answer in markdown and in 5 lines maximum.\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "QAcacTrryeWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_full(tokenizer, model, user_input)"
      ],
      "metadata": {
        "id": "wxEZYzNDvFeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_stream_low_level(tokenizer, model, user_input)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wlC7tJHOvFWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_stream_high_level(tokenizer, model, user_input)"
      ],
      "metadata": {
        "id": "GPlKGXq4PvGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the Gradio Interface"
      ],
      "metadata": {
        "id": "Il3deCOdvmC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the streaming function for gradio (using yield)\n",
        "def generate_stream(user_input):\n",
        "    # Global variables for modifications\n",
        "    global tokenizer, model, messages, max_tokens\n",
        "\n",
        "    # Step 1: Append the user's new message to the conversation history\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Step 2: Tokenize the input messages and convert them into a tensor\n",
        "    # - apply_chat_template: Formats the messages according to the model's expected input format.\n",
        "    # - return_tensors=\"pt\": Returns the result as a PyTorch tensor.\n",
        "    # - add_generation_prompt=True: Adds a special prompt or token for generation.\n",
        "    # - .to(\"cuda\"): Moves the tensor to the GPU for faster computation.\n",
        "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "    # Type: torch.Tensor of shape [batch_size, sequence_length].to(\"cuda\")\n",
        "\n",
        "    # Initialize an empty string to accumulate the generated result\n",
        "    result = \"\"\n",
        "\n",
        "    # Step 3: Start generating tokens in a loop, up to a maximum of 2000 tokens\n",
        "    for _ in range(max_tokens):\n",
        "        # Step 4: Pass the current input sequence to the model to predict the next token\n",
        "        # - outputs.logits: Contains the raw prediction scores (logits) for all possible tokens.\n",
        "        # - Shape of outputs.logits: [batch_size, sequence_length, vocab_size].\n",
        "        # - outputs.logits[:, -1]: Selects the logits for the last token position (shape: [batch_size, vocab_size]).\n",
        "        outputs = model(input_ids)\n",
        "        # Type: transformers.modeling_outputs.CausalLMOutputWithPast containing logits of shape [batch_size, sequence_length, vocab_size]\n",
        "\n",
        "        # Step 5: Find the token ID with the highest score (greedy decoding)\n",
        "        # - argmax(dim=-1): Selects the index of the maximum value along the vocab_size dimension.\n",
        "        # - unsqueeze(-1): Adds a new dimension at the last position, resulting in a shape of [batch_size, 1].\n",
        "        next_token_id = outputs.logits[:, -1].argmax(dim=-1).unsqueeze(-1)\n",
        "        # Type: torch.Tensor of shape [batch_size, 1].unsqueeze(-1)\n",
        "\n",
        "        # Step 6: Append the newly generated token ID to the input_ids tensor\n",
        "        # - torch.cat(): Concatenates the current input_ids with the next_token_id along the last dimension.\n",
        "        # - This updates input_ids to include the newly generated token, so the model can use the updated sequence in the next iteration.\n",
        "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
        "        # Type: torch.Tensor of shape [batch_size, updated_sequence_length]\n",
        "\n",
        "        # Step 7: Decode the newly generated token ID into a human-readable string\n",
        "        # - tokenizer.decode(): Converts the token ID into its corresponding string.\n",
        "        # - skip_special_tokens=True: Ensures special tokens like <eos> (end-of-sequence) are not included in the output.\n",
        "        next_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
        "        # Type: str representing the decoded token\n",
        "\n",
        "        # Step 8: Accumulate the decoded token into the result string\n",
        "        result += next_token\n",
        "\n",
        "        # Step 9: Yield the accumulated result for streaming output\n",
        "        # - yield allows the function to return partial results without stopping, enabling real-time streaming.\n",
        "        yield result\n",
        "\n",
        "        # Step 10: Check if the model predicted the end-of-sequence (EOS) token\n",
        "        # - tokenizer.eos_token_id: The special token ID representing EOS.\n",
        "        # - If EOS is detected, break the loop to stop further generation.\n",
        "        if next_token_id.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    # Append the final assistant response to the conversation history\n",
        "    messages.append({\"role\": \"assistant\", \"content\": result})"
      ],
      "metadata": {
        "id": "kbNTU0Fh-8Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimize the streaming function for gradio (using TextIteratorStreamer)\n",
        "def generate_stream_optimized(user_input):\n",
        "  # Global variables for modifications\n",
        "  global tokenizer, model, messages, max_tokens\n",
        "\n",
        "  # Step 1: Append the user's new message to the conversation history\n",
        "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "  # Step 2: Prepare the inputs for the model by applying the chat template\n",
        "  # The inputs include the conversation history and the user's latest message\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  # we skip using TextStreamer() here cause it streams back results to stdout and thats not what we want in gradio app\n",
        "  # we use TextIteratorStreamer() instead\n",
        "\n",
        "  # Step 3: Initialize the TextIteratorStreamer\n",
        "  streamer = TextIteratorStreamer(\n",
        "      tokenizer,\n",
        "      skip_prompt=True,  # Ensures that the input prompt is not repeatedly included in the streamed output.\n",
        "      decode_kwargs={\"skip_special_tokens\": True}  # Filters out special tokens (e.g., <s>, </s>, <pad>, <cls>, <sep>) from the generated text.\n",
        "  )\n",
        "\n",
        "  # Step 4: Create a thread to run the generation process in the background\n",
        "  thread = threading.Thread(\n",
        "      target=model.generate,  # Specifies that the model's `generate` method will be run in the thread.\n",
        "      kwargs={                           # Passes the arguments required for text generation\n",
        "          \"inputs\": inputs,              # The tokenized input prompt for the model.\n",
        "          \"max_new_tokens\": max_tokens,  # Limits the number of tokens to be generated.\n",
        "          \"streamer\": streamer           # The TextIteratorStreamer to handle streaming the output.\n",
        "          }\n",
        "  )\n",
        "\n",
        "  # Step 5: Start the thread to begin the generation process\n",
        "  thread.start()\n",
        "\n",
        "  # Step 6: Initialize an empty string to accumulate the growing output\n",
        "  accumulated_reply = \"\"\n",
        "\n",
        "  # Step 7: Stream the output progressively\n",
        "  for text_chunk in streamer:  # Iterate over each chunk of text streamed by the model\n",
        "      # Filter out any unexpected special tokens manually if they appear to ensure a clean output\n",
        "      # `<|eot_id|>` is a special token (e.g., end-of-text marker) that may still appear in some outputs\n",
        "      filtered_chunk = text_chunk.replace(\"<|eot_id|>\", \"\")\n",
        "\n",
        "      # Append the filtered chunk to the accumulated text that holds all the generated text seen so far\n",
        "      accumulated_reply += filtered_chunk\n",
        "\n",
        "      # Yield the accumulated text to the calling function/UI for progressive updates,\n",
        "      # ensuring the output is continuously refreshed with new content\n",
        "      yield accumulated_reply\n",
        "\n",
        "  # Step 8: Append the final assistant response to the conversation history\n",
        "  messages.append({\"role\": \"assistant\", \"content\": accumulated_reply})"
      ],
      "metadata": {
        "id": "Jm7689b4M2WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Chat with AI (Streaming Enabled)\")\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        user_input = gr.Textbox(label=\"Your message\", placeholder=\"Type something...\")\n",
        "        output_box = gr.Markdown(label=\"AI Response\", min_height=50)\n",
        "        send_button = gr.Button(\"Send\")\n",
        "\n",
        "    send_button.click(fn=generate_stream_optimized, inputs=user_input, outputs=output_box)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "S2hmF2KNtPQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0lgQAEuvz6Qt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
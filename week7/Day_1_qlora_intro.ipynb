{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsssBgWM_l0"
      },
      "source": [
        "## Predict Product Prices\n",
        "\n",
        "### Week 7 Day 1\n",
        "\n",
        "An introduction to LoRA and QLoRA\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d14UgrL83gcg"
      },
      "source": [
        "## A reminder of 2 important pro-tips for using Colab:\n",
        "\n",
        "**Pro-tip 1:**\n",
        "\n",
        "The top of every colab has some pip installs. You may receive errors from pip when you run this, such as:\n",
        "\n",
        "> gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
        "\n",
        "These pip compatibility errors can be safely ignored; and while it's tempting to try to fix them by changing version numbers, that will actually introduce real problems!\n",
        "\n",
        "**Pro-tip 2:**\n",
        "\n",
        "In the middle of running a Colab, you might get an error like this:\n",
        "\n",
        "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
        "\n",
        "This is a super-misleading error message! Please don't try changing versions of packages...\n",
        "\n",
        "This actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n",
        "\n",
        "1. Runtime menu >> Disconnect and delete runtime\n",
        "2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n",
        "3. Connect to a new T4 using the button at the top right\n",
        "4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n",
        "5. Rerun the cells in the colab, from the top down, starting with the pip installs\n",
        "\n",
        "And all should work great - otherwise, ask me!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "#!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "#!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n",
        "#!pip install -q datasets requests peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\"\n",
        "\n",
        "# Hyperparameters for QLoRA Fine-Tuning\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JArT3QAQAjx"
      },
      "source": [
        "### Log in to HuggingFace\n",
        "\n",
        "If you don't already have a HuggingFace account, visit https://huggingface.co to sign up and create a token.\n",
        "\n",
        "Then select the Secrets for this Notebook by clicking on the key icon in the left, and add a new secret called `HF_TOKEN` with the value as your token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "## Trying out different Quantization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElUPTnB28iNK"
      },
      "outputs": [],
      "source": [
        "# Load the Base Model without quantization\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VcoS_DY9YSG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6gJWw3r86KQ"
      },
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVf8hf6S88C9"
      },
      "outputs": [],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv-00uzNFPOe"
      },
      "source": [
        "## Restart your session!\n",
        "\n",
        "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (installs and imports and HuggingFace login) again.\n",
        "\n",
        "This is to clean out the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ycR0B4CzUSJ"
      },
      "outputs": [],
      "source": [
        "# Load the Base Model using 8 bit\n",
        "\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVErveOYFOXW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FDztdnv0RCq"
      },
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvqOxYfk0RnY"
      },
      "outputs": [],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb7-0OfSEycl"
      },
      "source": [
        "## Restart your session!\n",
        "\n",
        "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (imports and HuggingFace login) again.\n",
        "\n",
        "This is to clean out the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "outputs": [],
      "source": [
        "# Load the Tokenizer and the Base Model using 4 bit\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FfMJ2JbzEr3"
      },
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjp1EHH10WTb"
      },
      "outputs": [],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSpavkXo1KOI"
      },
      "outputs": [],
      "source": [
        "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3kXoy3w1oMF"
      },
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2PL1nlM1tM1"
      },
      "outputs": [],
      "source": [
        "fine_tuned_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIaqo-gyBQRh"
      },
      "outputs": [],
      "source": [
        "# Each of the Target Modules has 2 LoRA Adaptor matrices, called lora_A and lora_B\n",
        "# These are designed so that weights can be adapted by adding alpha * lora_A * lora_B\n",
        "# Let's count the number of weights using their dimensions:\n",
        "\n",
        "# See the matrix dimensions above\n",
        "lora_q_proj = 4096 * 32 + 4096 * 32\n",
        "lora_k_proj = 4096 * 32 + 1024 * 32\n",
        "lora_v_proj = 4096 * 32 + 1024 * 32\n",
        "lora_o_proj = 4096 * 32 + 4096 * 32\n",
        "\n",
        "# Each layer comes to\n",
        "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
        "\n",
        "# There are 32 layers\n",
        "params = lora_layer * 32\n",
        "\n",
        "# So the total size in MB is\n",
        "size = (params * 4) / 1_000_000\n",
        "\n",
        "print(f\"Total number of params: {params:,} and size {size:,.1f}MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKnCsfUQBG-P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’¾ Downloading Models to Run Locally\n",
        "\n",
        "There are two main approaches to run these models on your local computer:\n",
        "\n",
        "### Option 1: Download the Base Model (Llama 3.1 8B)\n",
        "\n",
        "The base model is **meta-llama/Meta-Llama-3.1-8B** which is about **16GB** unquantized (or **4-5GB** with 4-bit quantization).\n",
        "\n",
        "### Option 2: Download the Fine-tuned LoRA Adapter\n",
        "\n",
        "The LoRA adapter is much smaller (**~50MB**) and works on top of the base model.\n",
        "\n",
        "Both options are explained below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: Using Hugging Face CLI (Recommended)\n",
        "\n",
        "This is the easiest way to download models from Hugging Face Hub to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install huggingface-hub CLI (run this in your local terminal)\n",
        "# pip install huggingface-hub\n",
        "\n",
        "# Step 2: Login to Hugging Face (you'll need your HF token)\n",
        "# huggingface-cli login\n",
        "\n",
        "# Step 3: Download the base model (this will take a while - it's ~16GB)\n",
        "# huggingface-cli download meta-llama/Meta-Llama-3.1-8B --local-dir ./models/llama-3.1-8b\n",
        "\n",
        "# Step 4: Download the LoRA adapter (much smaller - ~50MB)\n",
        "# huggingface-cli download ed-donner/pricer-2024-09-13_13.04.39 --local-dir ./models/pricer-lora\n",
        "\n",
        "# The models will be saved to ./models/ directory\n",
        "# You can then load them locally without internet!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2: Download Programmatically with Python\n",
        "\n",
        "You can also download models using Python code, which gives you more control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# Download the base model\n",
        "model_path = snapshot_download(\n",
        "    repo_id=\"meta-llama/Meta-Llama-3.1-8B\",\n",
        "    local_dir=\"./models/llama-3.1-8b\",\n",
        "    token=\"your_hf_token_here\"  # or use HF_TOKEN from environment\n",
        ")\n",
        "\n",
        "print(f\"Base model downloaded to: {model_path}\")\n",
        "\n",
        "# Download the LoRA adapter\n",
        "lora_path = snapshot_download(\n",
        "    repo_id=\"ed-donner/pricer-2024-09-13_13.04.39\",\n",
        "    local_dir=\"./models/pricer-lora\",\n",
        "    token=\"your_hf_token_here\"\n",
        ")\n",
        "\n",
        "print(f\"LoRA adapter downloaded to: {lora_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Models Locally After Download\n",
        "\n",
        "Once you've downloaded the models, here's how to load them from your local directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./models/llama-3.1-8b\")\n",
        "\n",
        "# Load the base model with 4-bit quantization (to save memory)\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./models/llama-3.1-8b\",  # Local path instead of repo name\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Load the LoRA adapter on top of the base model\n",
        "model = PeftModel.from_pretrained(base_model, \"./models/pricer-lora\")\n",
        "\n",
        "print(\"Models loaded from local storage!\")\n",
        "print(f\"Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ’¡ Pro Tips for Local Usage\n",
        "\n",
        "**Storage Requirements:**\n",
        "- Base model (unquantized): ~16GB\n",
        "- Base model (4-bit quantized): ~4-5GB  \n",
        "- LoRA adapter: ~50MB\n",
        "\n",
        "**Memory Requirements (GPU/RAM):**\n",
        "- With 4-bit quantization: ~6GB VRAM minimum\n",
        "- With 8-bit quantization: ~10GB VRAM\n",
        "- Without quantization: ~20GB VRAM\n",
        "\n",
        "**Best Practices:**\n",
        "1. Always use quantization (4-bit recommended) to save memory\n",
        "2. Download LoRA adapter separately - it's tiny!\n",
        "3. Keep models in a dedicated `./models/` directory\n",
        "4. You can delete model files after downloading to save space\n",
        "5. Use `device_map=\"auto\"` to automatically distribute across GPU/CPU\n",
        "\n",
        "**Running Without GPU:**\n",
        "If you don't have a GPU, you can still run with CPU (it will be slower):\n",
        "```python\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./models/llama-3.1-8b\",\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"cpu\",  # Force CPU usage\n",
        ")\n",
        "```\n",
        "\n",
        "**Alternative: Use Ollama Locally**\n",
        "For easier local inference without Python, consider using Ollama:\n",
        "```bash\n",
        "# Install Ollama, then pull Llama 3.1\n",
        "ollama pull llama3.1:8b\n",
        "ollama run llama3.1:8b\n",
        "```\n",
        "This is simpler but doesn't support loading custom LoRA adapters directly."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
